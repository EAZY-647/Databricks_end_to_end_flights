# End-to-End Aviation Data Pipeline

## Project Overview

This project demonstrates a production-ready data engineering solution built on the Databricks Lakehouse Platform. It is designed to ingest high-volume, messy aviation dataâ€”including flight logs, passenger manifests, and airport codesâ€”and transform it into a structured Star Schema optimized for high-performance analytics.

The pipeline addresses a common industry challenge: moving data from raw file storage to business-ready insights while maintaining data quality, lineage, and scalability. It leverages modern Data Engineering practices including incremental ingestion, schema enforcement, and Slowly Changing Dimensions (SCD) modeling.

## Architecture
<img width="1408" height="752" alt="Gemini_Generated_Image_oipe9loipe9loipe" src="https://github.com/user-attachments/assets/c549ecc9-927f-40c6-abed-b6701ead7701" />


The solution follows the Medallion Architecture, dividing data flow into three logical layers to ensure quality and governance.

* ðŸ¥‰ **Bronze Layer (Raw Ingestion):**
    This layer handles the intake of raw data from cloud storage. It uses **Databricks Autoloader** (Spark Structured Streaming) to detect new JSON and CSV files as they arrive. This ensures exactly-once processing and automatically handles schema drift, preserving the original raw data for audit purposes.

* ðŸ¥ˆ **Silver Layer (Cleaned & Conformed):**
    Data from the Bronze layer is cleaned and standardized using **Delta Live Tables (DLT)** concepts. Key transformations in this stage include:
    * **Deduplication:** Removing duplicate records to ensure data integrity.
    * **Schema Enforcement:** Validating data types (e.g., converting string timestamps to proper timestamp objects).
    * **Standardization:** Renaming columns from inconsistent formats to snake_case for better readability.

* ðŸ¥‡ **Gold Layer (Dimensional Modeling):**
    The final layer implements a **Star Schema** designed for Business Intelligence tools like PowerBI, Tableau, or dbt.
    * **Fact Table:** `fact_bookings` contains transactional metrics such as booking amounts and flight references.
    * **Dimension Tables:** `dim_flights`, `dim_passengers`, and `dim_airports` provide descriptive context.
    * **SCD Type 1:** The pipeline implements logic to handle updates to dimension attributes, ensuring the data warehouse always reflects the most current state of the business.

## Pipeline Visualizations

### Transformation Logic
The cleaning and processing steps are visualized below. This graph represents the data lineage as it flows from the raw Bronze state through the validation logic of the Silver layer.

<img width="2878" height="1692" alt="databricks_dlt_silver layer" src="https://github.com/user-attachments/assets/a3fd89bc-e41f-4b84-b4b5-afeefd30ee9e" />

## Technical Stack

* **Platform:** Databricks Data Intelligence Platform
* **Storage Format:** Delta Lake (Open source storage layer with ACID transactions)
* **Compute Engine:** Apache Spark (PySpark & Spark SQL)
* **Ingestion:** Databricks Autoloader
* **Orchestration:** Databricks Workflows
* **Modeling Strategy:** Dimensional Modeling (Star Schema)

## Repository Structure

This repository contains the following notebooks that define the pipeline:

1.  **0_project_setup.py** (`setup.ipynb`)
    Initializes the environment by creating the Unity Catalog infrastructure. It sets up the Catalog, Schemas (databases), and Storage Volumes required for the project.

2.  **1_ingest_bronze.py** (`BronzeLayer.ipynb`)
    Configures the Autoloader streams. It defines the source location for raw files and the target Bronze Delta tables, enabling continuous ingestion.

3.  **2_transform_silver.py** (`SilverNotebook.ipynb`)
    Contains the core transformation logic. It reads from Bronze tables, applies data quality expectations (constraints), and writes cleaned data to the Silver layer.

4.  **3_build_gold_model.py** (`GOLD_DIMS.ipynb`)
    Implements the dimensional modeling logic. This script features a custom builder pattern that:
    * Reads clean Silver data.
    * Generates surrogate keys for dimensions.
    * Merges data (Upsert) to handle existing records vs. new records.
    * Joins dimensions to create the final Fact table.
<img width="1060" height="1730" alt="databricks_gold-layer" src="https://github.com/user-attachments/assets/2f2bf374-2243-44e0-84b2-ad19c4b4fc4c" />
<img width="2878" height="1695" alt="DBT_accessing the file from databricks" src="https://github.com/user-attachments/assets/597d2871-1e2c-4f49-945b-4a06ddb0169b" />

## Analytics & Results

The primary goal of this pipeline is to enable analytical reporting. By structuring data into a Fact and Dimension model, we can run complex aggregation queries efficiently.

**Example Business Query:**
*Analyze total revenue generated by each airline, broken down by departure city.*

```sql
SELECT 
    f.airline AS Airline_Name,
    a.city AS Departure_City,
    COUNT(b.booking_id) AS Total_Bookings,
    SUM(b.amount) AS Total_Revenue
FROM workspace.gold.fact_bookings b
JOIN workspace.gold.dim_flights f 
    ON b.dim_flights_key = f.dim_flights_key
JOIN workspace.gold.dim_airports a 
    ON b.dim_airports_key = a.dim_airports_key
GROUP BY f.airline, a.city
ORDER BY Total_Revenue DESC
LIMIT 5;
