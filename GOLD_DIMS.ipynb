{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90a30d5a-d2aa-4a92-b8db-f94f50c5847e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 1"
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>flight_id</th><th>airline</th><th>origin</th><th>destination</th><th>flight_date</th><th>modifiedDate</th></tr></thead><tbody><tr><td>F0001</td><td>Delta</td><td>Kellyfort</td><td>South Kathleen</td><td>2025-05-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0002</td><td>Qatar Airways</td><td>Lake Stephen</td><td>New Vincent</td><td>2025-04-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0003</td><td>Lufthansa</td><td>East Patrickborough</td><td>North Mary</td><td>2025-05-11</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0004</td><td>Delta</td><td>Maddenshire</td><td>Johnchester</td><td>2025-05-16</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0005</td><td>Qatar Airways</td><td>Bennettside</td><td>New Mistyhaven</td><td>2025-06-13</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0006</td><td>Air Canada</td><td>New Richardside</td><td>South Jamesborough</td><td>2025-05-16</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0007</td><td>Delta</td><td>Berryport</td><td>Miguelburgh</td><td>2025-05-24</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0008</td><td>Lufthansa</td><td>Briannachester</td><td>Cervantesland</td><td>2025-05-26</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0009</td><td>Delta</td><td>Alexandraborough</td><td>North Alexishaven</td><td>2025-06-10</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0010</td><td>Emirates</td><td>Kruegerchester</td><td>Martintown</td><td>2025-05-20</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0011</td><td>Lufthansa</td><td>Port Joannahaven</td><td>North Jasonton</td><td>2025-05-30</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0012</td><td>Air Canada</td><td>New Hunter</td><td>East Maria</td><td>2025-04-23</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0013</td><td>Air Canada</td><td>East Brian</td><td>Wayneton</td><td>2025-05-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0014</td><td>Air Canada</td><td>Toddland</td><td>Gonzalezport</td><td>2025-05-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0015</td><td>Delta</td><td>West Ginaton</td><td>Youngmouth</td><td>2025-05-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0016</td><td>Qatar Airways</td><td>Smithborough</td><td>Randallview</td><td>2025-05-02</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0017</td><td>Delta</td><td>Wallermouth</td><td>New Rogerberg</td><td>2025-05-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0018</td><td>Air Canada</td><td>Schultzborough</td><td>Danielton</td><td>2025-06-01</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0019</td><td>Lufthansa</td><td>Lake Scott</td><td>Christineborough</td><td>2025-05-18</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0020</td><td>Lufthansa</td><td>North Jack</td><td>Loweville</td><td>2025-06-02</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0021</td><td>Emirates</td><td>South Sara</td><td>North Annaport</td><td>2025-05-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0022</td><td>Air Canada</td><td>Reedchester</td><td>Cruzton</td><td>2025-04-27</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0023</td><td>Lufthansa</td><td>Port Jeremyfurt</td><td>Lake Walterside</td><td>2025-05-11</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0024</td><td>Lufthansa</td><td>Leestad</td><td>Port Debra</td><td>2025-05-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0025</td><td>Qatar Airways</td><td>Chelseaberg</td><td>North Michaelshire</td><td>2025-06-01</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0026</td><td>Air Canada</td><td>New Deborahfort</td><td>New Sabrina</td><td>2025-05-10</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0027</td><td>Delta</td><td>Acevedoville</td><td>Ericchester</td><td>2025-05-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0028</td><td>Delta</td><td>South Michaelberg</td><td>South Markbury</td><td>2025-05-30</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0029</td><td>Qatar Airways</td><td>Shortberg</td><td>Julieburgh</td><td>2025-05-11</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0030</td><td>Delta</td><td>Angelamouth</td><td>Sosaside</td><td>2025-06-16</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0031</td><td>Emirates</td><td>Billview</td><td>Jonathonborough</td><td>2025-06-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0032</td><td>Emirates</td><td>Bushhaven</td><td>Patriciamouth</td><td>2025-05-22</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0033</td><td>Air Canada</td><td>Sandersshire</td><td>Lake Joseside</td><td>2025-06-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0034</td><td>Emirates</td><td>Johnsonville</td><td>Cooperland</td><td>2025-05-18</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0035</td><td>Lufthansa</td><td>Karenshire</td><td>North Anthonymouth</td><td>2025-05-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0036</td><td>Lufthansa</td><td>Armstrongbury</td><td>North Natashamouth</td><td>2025-05-05</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0037</td><td>Air Canada</td><td>Yangville</td><td>Jacksonview</td><td>2025-04-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0038</td><td>Delta</td><td>Port Nicole</td><td>South Corey</td><td>2025-05-22</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0039</td><td>Air Canada</td><td>Paultown</td><td>Kellyton</td><td>2025-05-19</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0040</td><td>Delta</td><td>Ianport</td><td>Lake Reginaldside</td><td>2025-04-23</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0041</td><td>Air Canada</td><td>Lake Stanleyside</td><td>Port Scott</td><td>2025-06-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0042</td><td>Air Canada</td><td>Port William</td><td>Darrenmouth</td><td>2025-05-11</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0043</td><td>Lufthansa</td><td>Kimberlyborough</td><td>New Annfort</td><td>2025-05-15</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0044</td><td>Qatar Airways</td><td>East Kaylaburgh</td><td>Lake Carolynchester</td><td>2025-06-01</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0045</td><td>Emirates</td><td>North Michaelville</td><td>Russellton</td><td>2025-05-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0046</td><td>Qatar Airways</td><td>West David</td><td>North Becky</td><td>2025-04-30</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0047</td><td>Lufthansa</td><td>New Cory</td><td>Lorettashire</td><td>2025-05-05</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0048</td><td>Lufthansa</td><td>Cooperview</td><td>Michelleburgh</td><td>2025-05-24</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0049</td><td>Emirates</td><td>New Elizabethmouth</td><td>Lake Heathermouth</td><td>2025-05-06</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0050</td><td>Air Canada</td><td>East Gailtown</td><td>Stevenville</td><td>2025-04-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0051</td><td>Qatar Airways</td><td>Lisafurt</td><td>New Robert</td><td>2025-05-07</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0052</td><td>Qatar Airways</td><td>Lake Richard</td><td>Brookston</td><td>2025-05-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0053</td><td>Qatar Airways</td><td>West Dylanbury</td><td>North Wendy</td><td>2025-05-24</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0054</td><td>Air Canada</td><td>Seanmouth</td><td>North Kathyton</td><td>2025-05-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0055</td><td>Air Canada</td><td>Spencerborough</td><td>Randyville</td><td>2025-06-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0056</td><td>Lufthansa</td><td>South Natalie</td><td>Schaefertown</td><td>2025-05-03</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0057</td><td>Air Canada</td><td>Samanthashire</td><td>Fisherstad</td><td>2025-06-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0058</td><td>Delta</td><td>North Brittanyborough</td><td>West Kellystad</td><td>2025-06-10</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0059</td><td>Lufthansa</td><td>South Jeffrey</td><td>Jamesland</td><td>2025-05-10</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0060</td><td>Air Canada</td><td>Port Angelicaborough</td><td>East Daniel</td><td>2025-05-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0061</td><td>Lufthansa</td><td>Dawnshire</td><td>Stevenmouth</td><td>2025-04-24</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0062</td><td>Lufthansa</td><td>East Samanthafurt</td><td>Lake Troy</td><td>2025-05-12</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0063</td><td>Lufthansa</td><td>Floresshire</td><td>New Anita</td><td>2025-06-03</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0064</td><td>Delta</td><td>East Joshuastad</td><td>North Austinland</td><td>2025-05-25</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0065</td><td>Emirates</td><td>East Paulland</td><td>Barnesberg</td><td>2025-04-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0066</td><td>Qatar Airways</td><td>Stephenberg</td><td>Marcborough</td><td>2025-06-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0067</td><td>Delta</td><td>Campbellborough</td><td>South Jason</td><td>2025-05-13</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0068</td><td>Qatar Airways</td><td>Lake Seanport</td><td>Fieldsstad</td><td>2025-06-17</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0069</td><td>Qatar Airways</td><td>Johnsonburgh</td><td>Woodmouth</td><td>2025-06-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0070</td><td>Air Canada</td><td>Glennhaven</td><td>Port Darrenchester</td><td>2025-05-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0071</td><td>Delta</td><td>Jefferyberg</td><td>Wendyshire</td><td>2025-06-17</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0072</td><td>Emirates</td><td>Heiditown</td><td>Port Sabrinaburgh</td><td>2025-05-22</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0073</td><td>Lufthansa</td><td>South Jasonville</td><td>Clarkeshire</td><td>2025-06-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0074</td><td>Air Canada</td><td>Duffyville</td><td>Port Vanessa</td><td>2025-05-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0075</td><td>Qatar Airways</td><td>New Danielhaven</td><td>Cunninghamfurt</td><td>2025-04-27</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0076</td><td>Lufthansa</td><td>Johnsonchester</td><td>New Michael</td><td>2025-05-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0077</td><td>Qatar Airways</td><td>Samuelfort</td><td>Port Ashley</td><td>2025-06-12</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0078</td><td>Qatar Airways</td><td>Howardbury</td><td>Burchburgh</td><td>2025-05-09</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0079</td><td>Delta</td><td>Jonesburgh</td><td>Andradeville</td><td>2025-06-17</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0080</td><td>Emirates</td><td>Sonyaberg</td><td>East Sarah</td><td>2025-06-13</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0081</td><td>Air Canada</td><td>North Barry</td><td>Josephborough</td><td>2025-04-24</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0082</td><td>Qatar Airways</td><td>North Erikamouth</td><td>Danielfort</td><td>2025-05-19</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0083</td><td>Lufthansa</td><td>Jasonborough</td><td>Port Audreyville</td><td>2025-05-30</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0084</td><td>Delta</td><td>Franklinmouth</td><td>Lake Joseph</td><td>2025-05-30</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0085</td><td>Lufthansa</td><td>Hillton</td><td>Castroborough</td><td>2025-05-19</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0086</td><td>Delta</td><td>Port Chelsea</td><td>Robertchester</td><td>2025-05-08</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0087</td><td>Emirates</td><td>Kellyland</td><td>New Jonathanmouth</td><td>2025-04-23</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0088</td><td>Qatar Airways</td><td>North Miguel</td><td>Lake Allisonfurt</td><td>2025-05-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0089</td><td>Qatar Airways</td><td>Robinsonberg</td><td>Adrianchester</td><td>2025-05-21</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0090</td><td>Delta</td><td>East Rachelport</td><td>Davisport</td><td>2025-05-23</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0091</td><td>Lufthansa</td><td>Lake Scott</td><td>Williamshire</td><td>2025-06-04</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0092</td><td>Lufthansa</td><td>Austinborough</td><td>Bautistafort</td><td>2025-05-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0093</td><td>Air Canada</td><td>Frenchside</td><td>West Christopher</td><td>2025-05-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0094</td><td>Emirates</td><td>South Amyfurt</td><td>Jimenezview</td><td>2025-06-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0095</td><td>Qatar Airways</td><td>North Dawnberg</td><td>East Sarahmouth</td><td>2025-05-26</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0096</td><td>Lufthansa</td><td>Coreyfort</td><td>West Joseph</td><td>2025-06-06</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0097</td><td>Emirates</td><td>Billyport</td><td>Brentton</td><td>2025-05-18</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0098</td><td>Air Canada</td><td>Wallerburgh</td><td>Lake Karen</td><td>2025-05-01</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0099</td><td>Air Canada</td><td>Lake Connor</td><td>Matthewberg</td><td>2025-04-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0100</td><td>Qatar Airways</td><td>Port Anthony</td><td>Thompsonfort</td><td>2025-05-31</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0101</td><td>Qatar Airways</td><td>Lake Joshuamouth</td><td>Johnshire</td><td>2025-07-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0102</td><td>Delta</td><td>Schroederbury</td><td>Sethfort</td><td>2025-06-25</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0103</td><td>Qatar Airways</td><td>Jameschester</td><td>Ashleyberg</td><td>2025-07-15</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0104</td><td>Qatar Airways</td><td>Huangstad</td><td>Catherinehaven</td><td>2025-06-29</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0105</td><td>Air Canada</td><td>Aprilton</td><td>Victorville</td><td>2025-07-05</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0106</td><td>Lufthansa</td><td>East Austinmouth</td><td>New Toddburgh</td><td>2025-06-22</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0107</td><td>Air Canada</td><td>Mitchelltown</td><td>Johnsonbury</td><td>2025-07-01</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0108</td><td>Lufthansa</td><td>New Jennifer</td><td>Lake Tracie</td><td>2025-06-28</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0109</td><td>Air Canada</td><td>Williamsland</td><td>South Carolberg</td><td>2025-06-14</td><td>2026-02-09T06:19:05.812Z</td></tr><tr><td>F0110</td><td>Emirates</td><td>Churchport</td><td>Wrightton</td><td>2025-07-15</td><td>2026-02-09T06:19:05.812Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "F0001",
         "Delta",
         "Kellyfort",
         "South Kathleen",
         "2025-05-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0002",
         "Qatar Airways",
         "Lake Stephen",
         "New Vincent",
         "2025-04-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0003",
         "Lufthansa",
         "East Patrickborough",
         "North Mary",
         "2025-05-11",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0004",
         "Delta",
         "Maddenshire",
         "Johnchester",
         "2025-05-16",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0005",
         "Qatar Airways",
         "Bennettside",
         "New Mistyhaven",
         "2025-06-13",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0006",
         "Air Canada",
         "New Richardside",
         "South Jamesborough",
         "2025-05-16",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0007",
         "Delta",
         "Berryport",
         "Miguelburgh",
         "2025-05-24",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0008",
         "Lufthansa",
         "Briannachester",
         "Cervantesland",
         "2025-05-26",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0009",
         "Delta",
         "Alexandraborough",
         "North Alexishaven",
         "2025-06-10",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0010",
         "Emirates",
         "Kruegerchester",
         "Martintown",
         "2025-05-20",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0011",
         "Lufthansa",
         "Port Joannahaven",
         "North Jasonton",
         "2025-05-30",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0012",
         "Air Canada",
         "New Hunter",
         "East Maria",
         "2025-04-23",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0013",
         "Air Canada",
         "East Brian",
         "Wayneton",
         "2025-05-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0014",
         "Air Canada",
         "Toddland",
         "Gonzalezport",
         "2025-05-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0015",
         "Delta",
         "West Ginaton",
         "Youngmouth",
         "2025-05-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0016",
         "Qatar Airways",
         "Smithborough",
         "Randallview",
         "2025-05-02",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0017",
         "Delta",
         "Wallermouth",
         "New Rogerberg",
         "2025-05-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0018",
         "Air Canada",
         "Schultzborough",
         "Danielton",
         "2025-06-01",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0019",
         "Lufthansa",
         "Lake Scott",
         "Christineborough",
         "2025-05-18",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0020",
         "Lufthansa",
         "North Jack",
         "Loweville",
         "2025-06-02",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0021",
         "Emirates",
         "South Sara",
         "North Annaport",
         "2025-05-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0022",
         "Air Canada",
         "Reedchester",
         "Cruzton",
         "2025-04-27",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0023",
         "Lufthansa",
         "Port Jeremyfurt",
         "Lake Walterside",
         "2025-05-11",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0024",
         "Lufthansa",
         "Leestad",
         "Port Debra",
         "2025-05-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0025",
         "Qatar Airways",
         "Chelseaberg",
         "North Michaelshire",
         "2025-06-01",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0026",
         "Air Canada",
         "New Deborahfort",
         "New Sabrina",
         "2025-05-10",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0027",
         "Delta",
         "Acevedoville",
         "Ericchester",
         "2025-05-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0028",
         "Delta",
         "South Michaelberg",
         "South Markbury",
         "2025-05-30",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0029",
         "Qatar Airways",
         "Shortberg",
         "Julieburgh",
         "2025-05-11",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0030",
         "Delta",
         "Angelamouth",
         "Sosaside",
         "2025-06-16",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0031",
         "Emirates",
         "Billview",
         "Jonathonborough",
         "2025-06-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0032",
         "Emirates",
         "Bushhaven",
         "Patriciamouth",
         "2025-05-22",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0033",
         "Air Canada",
         "Sandersshire",
         "Lake Joseside",
         "2025-06-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0034",
         "Emirates",
         "Johnsonville",
         "Cooperland",
         "2025-05-18",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0035",
         "Lufthansa",
         "Karenshire",
         "North Anthonymouth",
         "2025-05-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0036",
         "Lufthansa",
         "Armstrongbury",
         "North Natashamouth",
         "2025-05-05",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0037",
         "Air Canada",
         "Yangville",
         "Jacksonview",
         "2025-04-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0038",
         "Delta",
         "Port Nicole",
         "South Corey",
         "2025-05-22",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0039",
         "Air Canada",
         "Paultown",
         "Kellyton",
         "2025-05-19",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0040",
         "Delta",
         "Ianport",
         "Lake Reginaldside",
         "2025-04-23",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0041",
         "Air Canada",
         "Lake Stanleyside",
         "Port Scott",
         "2025-06-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0042",
         "Air Canada",
         "Port William",
         "Darrenmouth",
         "2025-05-11",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0043",
         "Lufthansa",
         "Kimberlyborough",
         "New Annfort",
         "2025-05-15",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0044",
         "Qatar Airways",
         "East Kaylaburgh",
         "Lake Carolynchester",
         "2025-06-01",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0045",
         "Emirates",
         "North Michaelville",
         "Russellton",
         "2025-05-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0046",
         "Qatar Airways",
         "West David",
         "North Becky",
         "2025-04-30",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0047",
         "Lufthansa",
         "New Cory",
         "Lorettashire",
         "2025-05-05",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0048",
         "Lufthansa",
         "Cooperview",
         "Michelleburgh",
         "2025-05-24",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0049",
         "Emirates",
         "New Elizabethmouth",
         "Lake Heathermouth",
         "2025-05-06",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0050",
         "Air Canada",
         "East Gailtown",
         "Stevenville",
         "2025-04-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0051",
         "Qatar Airways",
         "Lisafurt",
         "New Robert",
         "2025-05-07",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0052",
         "Qatar Airways",
         "Lake Richard",
         "Brookston",
         "2025-05-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0053",
         "Qatar Airways",
         "West Dylanbury",
         "North Wendy",
         "2025-05-24",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0054",
         "Air Canada",
         "Seanmouth",
         "North Kathyton",
         "2025-05-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0055",
         "Air Canada",
         "Spencerborough",
         "Randyville",
         "2025-06-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0056",
         "Lufthansa",
         "South Natalie",
         "Schaefertown",
         "2025-05-03",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0057",
         "Air Canada",
         "Samanthashire",
         "Fisherstad",
         "2025-06-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0058",
         "Delta",
         "North Brittanyborough",
         "West Kellystad",
         "2025-06-10",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0059",
         "Lufthansa",
         "South Jeffrey",
         "Jamesland",
         "2025-05-10",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0060",
         "Air Canada",
         "Port Angelicaborough",
         "East Daniel",
         "2025-05-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0061",
         "Lufthansa",
         "Dawnshire",
         "Stevenmouth",
         "2025-04-24",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0062",
         "Lufthansa",
         "East Samanthafurt",
         "Lake Troy",
         "2025-05-12",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0063",
         "Lufthansa",
         "Floresshire",
         "New Anita",
         "2025-06-03",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0064",
         "Delta",
         "East Joshuastad",
         "North Austinland",
         "2025-05-25",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0065",
         "Emirates",
         "East Paulland",
         "Barnesberg",
         "2025-04-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0066",
         "Qatar Airways",
         "Stephenberg",
         "Marcborough",
         "2025-06-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0067",
         "Delta",
         "Campbellborough",
         "South Jason",
         "2025-05-13",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0068",
         "Qatar Airways",
         "Lake Seanport",
         "Fieldsstad",
         "2025-06-17",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0069",
         "Qatar Airways",
         "Johnsonburgh",
         "Woodmouth",
         "2025-06-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0070",
         "Air Canada",
         "Glennhaven",
         "Port Darrenchester",
         "2025-05-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0071",
         "Delta",
         "Jefferyberg",
         "Wendyshire",
         "2025-06-17",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0072",
         "Emirates",
         "Heiditown",
         "Port Sabrinaburgh",
         "2025-05-22",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0073",
         "Lufthansa",
         "South Jasonville",
         "Clarkeshire",
         "2025-06-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0074",
         "Air Canada",
         "Duffyville",
         "Port Vanessa",
         "2025-05-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0075",
         "Qatar Airways",
         "New Danielhaven",
         "Cunninghamfurt",
         "2025-04-27",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0076",
         "Lufthansa",
         "Johnsonchester",
         "New Michael",
         "2025-05-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0077",
         "Qatar Airways",
         "Samuelfort",
         "Port Ashley",
         "2025-06-12",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0078",
         "Qatar Airways",
         "Howardbury",
         "Burchburgh",
         "2025-05-09",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0079",
         "Delta",
         "Jonesburgh",
         "Andradeville",
         "2025-06-17",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0080",
         "Emirates",
         "Sonyaberg",
         "East Sarah",
         "2025-06-13",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0081",
         "Air Canada",
         "North Barry",
         "Josephborough",
         "2025-04-24",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0082",
         "Qatar Airways",
         "North Erikamouth",
         "Danielfort",
         "2025-05-19",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0083",
         "Lufthansa",
         "Jasonborough",
         "Port Audreyville",
         "2025-05-30",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0084",
         "Delta",
         "Franklinmouth",
         "Lake Joseph",
         "2025-05-30",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0085",
         "Lufthansa",
         "Hillton",
         "Castroborough",
         "2025-05-19",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0086",
         "Delta",
         "Port Chelsea",
         "Robertchester",
         "2025-05-08",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0087",
         "Emirates",
         "Kellyland",
         "New Jonathanmouth",
         "2025-04-23",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0088",
         "Qatar Airways",
         "North Miguel",
         "Lake Allisonfurt",
         "2025-05-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0089",
         "Qatar Airways",
         "Robinsonberg",
         "Adrianchester",
         "2025-05-21",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0090",
         "Delta",
         "East Rachelport",
         "Davisport",
         "2025-05-23",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0091",
         "Lufthansa",
         "Lake Scott",
         "Williamshire",
         "2025-06-04",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0092",
         "Lufthansa",
         "Austinborough",
         "Bautistafort",
         "2025-05-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0093",
         "Air Canada",
         "Frenchside",
         "West Christopher",
         "2025-05-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0094",
         "Emirates",
         "South Amyfurt",
         "Jimenezview",
         "2025-06-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0095",
         "Qatar Airways",
         "North Dawnberg",
         "East Sarahmouth",
         "2025-05-26",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0096",
         "Lufthansa",
         "Coreyfort",
         "West Joseph",
         "2025-06-06",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0097",
         "Emirates",
         "Billyport",
         "Brentton",
         "2025-05-18",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0098",
         "Air Canada",
         "Wallerburgh",
         "Lake Karen",
         "2025-05-01",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0099",
         "Air Canada",
         "Lake Connor",
         "Matthewberg",
         "2025-04-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0100",
         "Qatar Airways",
         "Port Anthony",
         "Thompsonfort",
         "2025-05-31",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0101",
         "Qatar Airways",
         "Lake Joshuamouth",
         "Johnshire",
         "2025-07-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0102",
         "Delta",
         "Schroederbury",
         "Sethfort",
         "2025-06-25",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0103",
         "Qatar Airways",
         "Jameschester",
         "Ashleyberg",
         "2025-07-15",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0104",
         "Qatar Airways",
         "Huangstad",
         "Catherinehaven",
         "2025-06-29",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0105",
         "Air Canada",
         "Aprilton",
         "Victorville",
         "2025-07-05",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0106",
         "Lufthansa",
         "East Austinmouth",
         "New Toddburgh",
         "2025-06-22",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0107",
         "Air Canada",
         "Mitchelltown",
         "Johnsonbury",
         "2025-07-01",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0108",
         "Lufthansa",
         "New Jennifer",
         "Lake Tracie",
         "2025-06-28",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0109",
         "Air Canada",
         "Williamsland",
         "South Carolberg",
         "2025-06-14",
         "2026-02-09T06:19:05.812Z"
        ],
        [
         "F0110",
         "Emirates",
         "Churchport",
         "Wrightton",
         "2025-07-15",
         "2026-02-09T06:19:05.812Z"
        ]
       ],
       "datasetInfos": [
        {
         "name": "_sqldf",
         "schema": {
          "fields": [
           {
            "metadata": {},
            "name": "flight_id",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "airline",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "origin",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "destination",
            "nullable": true,
            "type": "string"
           },
           {
            "metadata": {},
            "name": "flight_date",
            "nullable": true,
            "type": "date"
           },
           {
            "metadata": {},
            "name": "modifiedDate",
            "nullable": true,
            "type": "timestamp"
           }
          ],
          "type": "struct"
         },
         "tableIdentifier": null,
         "typeStr": "pyspark.sql.connect.dataframe.DataFrame"
        }
       ],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "createTempViewForImplicitDf": true,
        "dataframeName": "_sqldf",
        "executionCount": 4
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "flight_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "airline",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "origin",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "destination",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "flight_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "modifiedDate",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "SELECT * FROM workspace.default.silver_flights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee75afc5-c52e-4f73-9d88-42fa7716b4a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "['flight_id']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val = 'flight_id'\n",
    "eval(f\"['{val}']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "107142d5-241a-4c6e-9b99-1a6222370f70",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Cell 3"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading from: workspace.default.silver_flights\nWriting to:   workspace.gold.dim_flights\nCDC Column:   modifiedDate\nSuccess! Found 110 rows in source.\nProcessing data newer than: 1900-01-01 00:00:00\nTable workspace.gold.dim_flights created with 110 rows.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION (Fixed Column Name)\n",
    "# ==========================================\n",
    "key_columns_list = [\"flight_id\"] \n",
    "\n",
    "# CHANGED: Updated to match your table's actual column name\n",
    "cdc_column = \"modifiedDate\" \n",
    "\n",
    "source_schema = \"default\" \n",
    "source_table = \"silver_flights\"\n",
    "\n",
    "target_schema = \"gold\"\n",
    "target_table = \"dim_flights\"\n",
    "surrogate_key = \"dim_flights_key\"\n",
    "\n",
    "catalog = \"workspace\"\n",
    "\n",
    "# Construct full paths\n",
    "source_table_path = f\"{catalog}.{source_schema}.{source_table}\"\n",
    "target_table_path = f\"{catalog}.{target_schema}.{target_table}\"\n",
    "\n",
    "print(f\"Reading from: {source_table_path}\")\n",
    "print(f\"Writing to:   {target_table_path}\")\n",
    "print(f\"CDC Column:   {cdc_column}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. CHECK SOURCE DATA\n",
    "# ==========================================\n",
    "try:\n",
    "    df_source_check = spark.table(source_table_path)\n",
    "    print(f\"Success! Found {df_source_check.count()} rows in source.\")\n",
    "except Exception as e:\n",
    "    print(f\"CRITICAL ERROR: Could not find {source_table_path}.\")\n",
    "    dbutils.notebook.exit(\"Source table missing\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. DETERMINE LOAD DATE\n",
    "# ==========================================\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    try:\n",
    "        max_date_row = spark.sql(f\"SELECT MAX({cdc_column}) FROM {target_table_path}\").collect()[0][0]\n",
    "        last_load_date = max_date_row if max_date_row else \"1900-01-01 00:00:00\"\n",
    "    except:\n",
    "        last_load_date = \"1900-01-01 00:00:00\"\n",
    "else:\n",
    "    last_load_date = \"1900-01-01 00:00:00\"\n",
    "\n",
    "print(f\"Processing data newer than: {last_load_date}\")\n",
    "\n",
    "# ==========================================\n",
    "# 4. READ & FILTER SOURCE\n",
    "# ==========================================\n",
    "df_source = spark.table(source_table_path).filter(col(cdc_column) > lit(last_load_date))\n",
    "\n",
    "if df_source.count() == 0:\n",
    "    print(\"No new data to process.\")\n",
    "else:\n",
    "    # ==========================================\n",
    "    # 5. PREPARE JOIN LOGIC\n",
    "    # ==========================================\n",
    "    df_source.createOrReplaceTempView(\"source_view\")\n",
    "    \n",
    "    if spark.catalog.tableExists(target_table_path):\n",
    "        spark.table(target_table_path).createOrReplaceTempView(\"target_view\")\n",
    "    else:\n",
    "        # Create empty temp view if table doesn't exist\n",
    "        cols = \", \".join([f\"CAST(NULL AS STRING) AS {c}\" for c in key_columns_list])\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT {cols}, CAST(NULL AS LONG) AS {surrogate_key}, \n",
    "            CAST(NULL AS TIMESTAMP) AS create_date, \n",
    "            CAST(NULL AS TIMESTAMP) AS update_date \n",
    "            WHERE 1=0\n",
    "        \"\"\").createOrReplaceTempView(\"target_view\")\n",
    "\n",
    "    join_cond = \" AND \".join([f\"source_view.{c} = target_view.{c}\" for c in key_columns_list])\n",
    "    \n",
    "    df_joined = spark.sql(f\"\"\"\n",
    "        SELECT source_view.*, \n",
    "               target_view.{surrogate_key} as existing_key,\n",
    "               target_view.create_date as existing_create_date\n",
    "        FROM source_view\n",
    "        LEFT JOIN target_view ON {join_cond}\n",
    "    \"\"\")\n",
    "\n",
    "    df_new = df_joined.filter(\"existing_key IS NULL\")\n",
    "    df_old = df_joined.filter(\"existing_key IS NOT NULL\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 6. ENRICHMENT & KEY GENERATION\n",
    "    # ==========================================\n",
    "    if spark.catalog.tableExists(target_table_path):\n",
    "        mx = spark.sql(f\"SELECT MAX({surrogate_key}) FROM {target_table_path}\").collect()[0][0]\n",
    "        max_key = mx if mx else 0\n",
    "    else:\n",
    "        max_key = 0\n",
    "\n",
    "    df_new_enriched = df_new.withColumn(surrogate_key, (monotonically_increasing_id() + 1 + max_key).cast(\"long\")) \\\n",
    "                            .withColumn(\"create_date\", current_timestamp()) \\\n",
    "                            .withColumn(\"update_date\", current_timestamp()) \\\n",
    "                            .drop(\"existing_key\", \"existing_create_date\")\n",
    "\n",
    "    df_old_enriched = df_old.withColumn(surrogate_key, col(\"existing_key\").cast(\"long\")) \\\n",
    "                            .withColumn(\"create_date\", col(\"existing_create_date\")) \\\n",
    "                            .withColumn(\"update_date\", current_timestamp()) \\\n",
    "                            .drop(\"existing_key\", \"existing_create_date\")\n",
    "\n",
    "    # ==========================================\n",
    "    # 7. MERGE (UPSERT)\n",
    "    # ==========================================\n",
    "    df_final = df_new_enriched.unionByName(df_old_enriched)\n",
    "\n",
    "    # Ensure Gold schema exists\n",
    "    spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {catalog}.{target_schema}\")\n",
    "\n",
    "    if spark.catalog.tableExists(target_table_path):\n",
    "        dt = DeltaTable.forName(spark, target_table_path)\n",
    "        dt.alias(\"t\").merge(df_final.alias(\"s\"), f\"t.{surrogate_key} = s.{surrogate_key}\") \\\n",
    "            .whenMatchedUpdateAll(condition = f\"s.{cdc_column} >= t.{cdc_column}\") \\\n",
    "            .whenNotMatchedInsertAll() \\\n",
    "            .execute()\n",
    "        print(f\"Upsert Complete. {df_final.count()} rows processed.\")\n",
    "    else:\n",
    "        df_final.write.format(\"delta\").saveAsTable(target_table_path)\n",
    "        print(f\"Table {target_table_path} created with {df_final.count()} rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "025e54d0-e94e-43db-81d6-fcf633af3a0b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration for Airports ---\n",
    "key_columns_list = [\"airport_id\"] \n",
    "cdc_column = \"modifiedDate\"\n",
    "\n",
    "source_schema = \"default\"\n",
    "source_table = \"silver_airports\"\n",
    "\n",
    "target_schema = \"gold\"\n",
    "target_table = \"dim_airports\"\n",
    "surrogate_key = \"dim_airports_key\"\n",
    "\n",
    "catalog = \"workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f8001865-79b0-479b-9875-2976441f1f07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration for Passengers ---\n",
    "key_columns_list = [\"passenger_id\"] \n",
    "cdc_column = \"modifiedDate\"\n",
    "\n",
    "source_schema = \"default\"\n",
    "source_table = \"silver_passengers\"\n",
    "\n",
    "target_schema = \"gold\"\n",
    "target_table = \"dim_passengers\"\n",
    "surrogate_key = \"dim_passengers_key\"\n",
    "\n",
    "catalog = \"workspace\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3bfa312d-d56c-4212-b70b-4f96f293090d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Fact Table: workspace.gold.fact_bookings\nWriting data to Gold Layer...\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n",
       "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)\n",
       "File \u001B[0;32m<command-4791962059803034>, line 62\u001B[0m\n",
       "\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# ==========================================\u001B[39;00m\n",
       "\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# 4. WRITE TO GOLD (Fact Table)\u001B[39;00m\n",
       "\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# ==========================================\u001B[39;00m\n",
       "\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Fact tables are usually large, so we append or overwrite. \u001B[39;00m\n",
       "\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# For this project, we will overwrite to ensure a clean state.\u001B[39;00m\n",
       "\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting data to Gold Layer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     59\u001B[0m df_fact\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     60\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[1;32m     61\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n",
       "\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(fact_table)\n",
       "\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccess! Fact Table created at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfact_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m     65\u001B[0m display(spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfact_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m LIMIT 5\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n",
       "\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n",
       "\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n",
       "\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n",
       "\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n",
       "\u001B[1;32m    739\u001B[0m )\n",
       "\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n",
       "\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n",
       "\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n",
       "\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n",
       "\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n",
       "\u001B[1;32m   1558\u001B[0m )\n",
       "\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n",
       "\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n",
       "\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n",
       "\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n",
       "\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n",
       "\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n",
       "\u001B[1;32m   2061\u001B[0m     ):\n",
       "\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n",
       "\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n",
       "\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n",
       "\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n",
       "\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n",
       "\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
       "\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n",
       "\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n",
       "\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n",
       "\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
       "\n",
       "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n",
       "\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
       "\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n",
       "\u001B[1;32m   2434\u001B[0m                 info,\n",
       "\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n",
       "\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n",
       "\u001B[1;32m   2438\u001B[0m                 status_code,\n",
       "\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n",
       "\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n",
       "\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n",
       "\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n",
       "\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
       "\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
       "\n",
       "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`gold`.`dim_passengers` cannot be found. Verify the spelling and correctness of the schema and catalog.\n",
       "If you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\n",
       "To tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n",
       "'Project ['b.booking_id, 'b.booking_date, 'b.amount, 'f.dim_flights_key, 'p.dim_passengers_key, 'a.dim_airports_key, 'b.modifiedDate AS modify_date#14162]\n",
       "+- 'Join UsingJoin(LeftOuter, [airport_id])\n",
       "   :- 'Join UsingJoin(LeftOuter, [passenger_id])\n",
       "   :  :- Project [flight_id#14172, booking_id#14170, passenger_id#14171, airport_id#14173, amount#14174, booking_date#14175, modifiedDate#14176, dim_flights_key#14192L]\n",
       "   :  :  +- Join LeftOuter, (flight_id#14172 = flight_id#14186)\n",
       "   :  :     :- SubqueryAlias b\n",
       "   :  :     :  +- SubqueryAlias workspace.default.silver_bookings\n",
       "   :  :     :     +- Project [booking_id#14197 AS booking_id#14170, passenger_id#14198 AS passenger_id#14171, flight_id#14199 AS flight_id#14172, airport_id#14200 AS airport_id#14173, amount#14201 AS amount#14174, booking_date#14202 AS booking_date#14175, modifiedDate#14203 AS modifiedDate#14176]\n",
       "   :  :     :        +- View (`workspace`.`default`.`silver_bookings`, [booking_id#14197, passenger_id#14198, flight_id#14199, airport_id#14200, amount#14201, booking_date#14202, modifiedDate#14203])\n",
       "   :  :     :           +- Project [cast(booking_id#14204 as string) AS booking_id#14197, cast(passenger_id#14205 as string) AS passenger_id#14198, cast(flight_id#14206 as string) AS flight_id#14199, cast(airport_id#14207 as string) AS airport_id#14200, cast(amount#14208 as double) AS amount#14201, cast(booking_date#14209 as date) AS booking_date#14202, cast(modifiedDate#14210 as timestamp) AS modifiedDate#14203]\n",
       "   :  :     :              +- Project [booking_id#14204, passenger_id#14205, flight_id#14206, airport_id#14207, amount#14208, booking_date#14209, modifiedDate#14210]\n",
       "   :  :     :                 +- Relation workspace.default.silver_bookings[booking_id#14204,passenger_id#14205,flight_id#14206,airport_id#14207,amount#14208,booking_date#14209,modifiedDate#14210] parquet\n",
       "   :  :     +- SubqueryAlias f\n",
       "   :  :        +- Project [flight_id#14186, dim_flights_key#14192L]\n",
       "   :  :           +- SubqueryAlias workspace.gold.dim_flights\n",
       "   :  :              +- Relation workspace.gold.dim_flights[flight_id#14186,airline#14187,origin#14188,destination#14189,flight_date#14190,modifiedDate#14191,dim_flights_key#14192L,create_date#14193,update_date#14194] parquet\n",
       "   :  +- 'SubqueryAlias p\n",
       "   :     +- 'Project ['passenger_id, 'dim_passengers_key]\n",
       "   :        +- 'UnresolvedRelation [workspace, gold, dim_passengers], [], false\n",
       "   +- 'SubqueryAlias a\n",
       "      +- 'Project ['airport_id, 'dim_airports_key]\n",
       "         +- 'UnresolvedRelation [workspace, gold, dim_airports], [], false\n",
       "\n",
       "\n",
       "JVM stacktrace:\n",
       "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n",
       "\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.RelationResolution$.throwTableNotFound(RelationResolution.scala:1017)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:355)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n",
       "\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n",
       "\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n",
       "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n",
       "\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n",
       "\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n",
       "\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n",
       "\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n",
       "\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n",
       "\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n",
       "\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n",
       "\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n",
       "\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n",
       "\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n",
       "\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n",
       "\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n",
       "\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$13(Dataset.scala:257)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n",
       "\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:253)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4130)\n",
       "\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3515)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:290)\n",
       "\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:247)\n",
       "\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:536)\n",
       "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n",
       "\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n",
       "\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n",
       "\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n",
       "\tat scala.Option.map(Option.scala:242)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n",
       "\tat scala.util.Try$.apply(Try.scala:217)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n",
       "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n",
       "\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n",
       "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n",
       "\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n",
       "\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n",
       "\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n",
       "\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n",
       "\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n",
       "\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n",
       "\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n",
       "\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n",
       "\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "datasetInfos": [],
       "jupyterProps": {
        "ename": "AnalysisException",
        "evalue": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`gold`.`dim_passengers` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n'Project ['b.booking_id, 'b.booking_date, 'b.amount, 'f.dim_flights_key, 'p.dim_passengers_key, 'a.dim_airports_key, 'b.modifiedDate AS modify_date#14162]\n+- 'Join UsingJoin(LeftOuter, [airport_id])\n   :- 'Join UsingJoin(LeftOuter, [passenger_id])\n   :  :- Project [flight_id#14172, booking_id#14170, passenger_id#14171, airport_id#14173, amount#14174, booking_date#14175, modifiedDate#14176, dim_flights_key#14192L]\n   :  :  +- Join LeftOuter, (flight_id#14172 = flight_id#14186)\n   :  :     :- SubqueryAlias b\n   :  :     :  +- SubqueryAlias workspace.default.silver_bookings\n   :  :     :     +- Project [booking_id#14197 AS booking_id#14170, passenger_id#14198 AS passenger_id#14171, flight_id#14199 AS flight_id#14172, airport_id#14200 AS airport_id#14173, amount#14201 AS amount#14174, booking_date#14202 AS booking_date#14175, modifiedDate#14203 AS modifiedDate#14176]\n   :  :     :        +- View (`workspace`.`default`.`silver_bookings`, [booking_id#14197, passenger_id#14198, flight_id#14199, airport_id#14200, amount#14201, booking_date#14202, modifiedDate#14203])\n   :  :     :           +- Project [cast(booking_id#14204 as string) AS booking_id#14197, cast(passenger_id#14205 as string) AS passenger_id#14198, cast(flight_id#14206 as string) AS flight_id#14199, cast(airport_id#14207 as string) AS airport_id#14200, cast(amount#14208 as double) AS amount#14201, cast(booking_date#14209 as date) AS booking_date#14202, cast(modifiedDate#14210 as timestamp) AS modifiedDate#14203]\n   :  :     :              +- Project [booking_id#14204, passenger_id#14205, flight_id#14206, airport_id#14207, amount#14208, booking_date#14209, modifiedDate#14210]\n   :  :     :                 +- Relation workspace.default.silver_bookings[booking_id#14204,passenger_id#14205,flight_id#14206,airport_id#14207,amount#14208,booking_date#14209,modifiedDate#14210] parquet\n   :  :     +- SubqueryAlias f\n   :  :        +- Project [flight_id#14186, dim_flights_key#14192L]\n   :  :           +- SubqueryAlias workspace.gold.dim_flights\n   :  :              +- Relation workspace.gold.dim_flights[flight_id#14186,airline#14187,origin#14188,destination#14189,flight_date#14190,modifiedDate#14191,dim_flights_key#14192L,create_date#14193,update_date#14194] parquet\n   :  +- 'SubqueryAlias p\n   :     +- 'Project ['passenger_id, 'dim_passengers_key]\n   :        +- 'UnresolvedRelation [workspace, gold, dim_passengers], [], false\n   +- 'SubqueryAlias a\n      +- 'Project ['airport_id, 'dim_airports_key]\n         +- 'UnresolvedRelation [workspace, gold, dim_airports], [], false\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.RelationResolution$.throwTableNotFound(RelationResolution.scala:1017)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:355)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$13(Dataset.scala:257)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4130)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3515)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:290)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:247)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:536)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       },
       "metadata": {
        "errorSummary": "[TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`gold`.`dim_passengers` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01"
       },
       "removedWidgets": [],
       "sqlProps": {
        "breakingChangeInfo": null,
        "errorClass": "TABLE_OR_VIEW_NOT_FOUND",
        "pysparkCallSite": "",
        "pysparkFragment": "",
        "pysparkSummary": "",
        "sqlState": "42P01",
        "stackTrace": "org.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.RelationResolution$.throwTableNotFound(RelationResolution.scala:1017)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:355)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$13(Dataset.scala:257)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4130)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3515)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:290)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:247)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:536)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)",
        "startIndex": null,
        "stopIndex": null
       },
       "stackFrames": [
        "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
        "\u001B[0;31mAnalysisException\u001B[0m                         Traceback (most recent call last)",
        "File \u001B[0;32m<command-4791962059803034>, line 62\u001B[0m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# ==========================================\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;66;03m# 4. WRITE TO GOLD (Fact Table)\u001B[39;00m\n\u001B[1;32m     53\u001B[0m \u001B[38;5;66;03m# ==========================================\u001B[39;00m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;66;03m# Fact tables are usually large, so we append or overwrite. \u001B[39;00m\n\u001B[1;32m     55\u001B[0m \u001B[38;5;66;03m# For this project, we will overwrite to ensure a clean state.\u001B[39;00m\n\u001B[1;32m     57\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mWriting data to Gold Layer...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     59\u001B[0m df_fact\u001B[38;5;241m.\u001B[39mwrite\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mdelta\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     60\u001B[0m     \u001B[38;5;241m.\u001B[39mmode(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[1;32m     61\u001B[0m     \u001B[38;5;241m.\u001B[39moption(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverwriteSchema\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtrue\u001B[39m\u001B[38;5;124m\"\u001B[39m) \\\n\u001B[0;32m---> 62\u001B[0m     \u001B[38;5;241m.\u001B[39msaveAsTable(fact_table)\n\u001B[1;32m     64\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSuccess! Fact Table created at \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfact_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     65\u001B[0m display(spark\u001B[38;5;241m.\u001B[39msql(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mSELECT * FROM \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfact_table\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m LIMIT 5\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/readwriter.py:737\u001B[0m, in \u001B[0;36mDataFrameWriter.saveAsTable\u001B[0;34m(self, name, format, mode, partitionBy, **options)\u001B[0m\n\u001B[1;32m    735\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_name \u001B[38;5;241m=\u001B[39m name\n\u001B[1;32m    736\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mtable_save_method \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msave_as_table\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m--> 737\u001B[0m _, _, ei \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient\u001B[38;5;241m.\u001B[39mexecute_command(\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mcommand(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_spark\u001B[38;5;241m.\u001B[39mclient), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_write\u001B[38;5;241m.\u001B[39mobservations\n\u001B[1;32m    739\u001B[0m )\n\u001B[1;32m    740\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_callback(ei)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:1556\u001B[0m, in \u001B[0;36mSparkConnectClient.execute_command\u001B[0;34m(self, command, observations, extra_request_metadata)\u001B[0m\n\u001B[1;32m   1554\u001B[0m     req\u001B[38;5;241m.\u001B[39muser_context\u001B[38;5;241m.\u001B[39muser_id \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_user_id\n\u001B[1;32m   1555\u001B[0m req\u001B[38;5;241m.\u001B[39mplan\u001B[38;5;241m.\u001B[39mcommand\u001B[38;5;241m.\u001B[39mCopyFrom(command)\n\u001B[0;32m-> 1556\u001B[0m data, _, metrics, observed_metrics, properties \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch(\n\u001B[1;32m   1557\u001B[0m     req, observations \u001B[38;5;129;01mor\u001B[39;00m {}, extra_request_metadata\n\u001B[1;32m   1558\u001B[0m )\n\u001B[1;32m   1559\u001B[0m \u001B[38;5;66;03m# Create a query execution object.\u001B[39;00m\n\u001B[1;32m   1560\u001B[0m ei \u001B[38;5;241m=\u001B[39m ExecutionInfo(metrics, observed_metrics)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2059\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch\u001B[0;34m(self, req, observations, extra_request_metadata, self_destruct)\u001B[0m\n\u001B[1;32m   2056\u001B[0m properties: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m {}\n\u001B[1;32m   2058\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m Progress(handlers\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_progress_handlers, operation_id\u001B[38;5;241m=\u001B[39mreq\u001B[38;5;241m.\u001B[39moperation_id) \u001B[38;5;28;01mas\u001B[39;00m progress:\n\u001B[0;32m-> 2059\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m response \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_execute_and_fetch_as_iterator(\n\u001B[1;32m   2060\u001B[0m         req, observations, extra_request_metadata \u001B[38;5;129;01mor\u001B[39;00m [], progress\u001B[38;5;241m=\u001B[39mprogress\n\u001B[1;32m   2061\u001B[0m     ):\n\u001B[1;32m   2062\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(response, StructType):\n\u001B[1;32m   2063\u001B[0m             schema \u001B[38;5;241m=\u001B[39m response\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2035\u001B[0m, in \u001B[0;36mSparkConnectClient._execute_and_fetch_as_iterator\u001B[0;34m(self, req, observations, extra_request_metadata, progress)\u001B[0m\n\u001B[1;32m   2033\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m kb\n\u001B[1;32m   2034\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m error:\n\u001B[0;32m-> 2035\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_error(error)\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2355\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_error\u001B[0;34m(self, error)\u001B[0m\n\u001B[1;32m   2353\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mthread_local\u001B[38;5;241m.\u001B[39minside_error_handling \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m   2354\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(error, grpc\u001B[38;5;241m.\u001B[39mRpcError):\n\u001B[0;32m-> 2355\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error(error)\n\u001B[1;32m   2356\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m   2357\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n",
        "File \u001B[0;32m/databricks/python/lib/python3.12/site-packages/pyspark/sql/connect/client/core.py:2433\u001B[0m, in \u001B[0;36mSparkConnectClient._handle_rpc_error\u001B[0;34m(self, rpc_error)\u001B[0m\n\u001B[1;32m   2429\u001B[0m             logger\u001B[38;5;241m.\u001B[39mdebug(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mReceived ErrorInfo: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00minfo\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   2431\u001B[0m             \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_handle_rpc_error_with_error_info(info, status\u001B[38;5;241m.\u001B[39mmessage, status_code)  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[0;32m-> 2433\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m convert_exception(\n\u001B[1;32m   2434\u001B[0m                 info,\n\u001B[1;32m   2435\u001B[0m                 status\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2436\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_fetch_enriched_error(info),\n\u001B[1;32m   2437\u001B[0m                 \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_display_server_stack_trace(),\n\u001B[1;32m   2438\u001B[0m                 status_code,\n\u001B[1;32m   2439\u001B[0m             ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2441\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m SparkConnectGrpcException(\n\u001B[1;32m   2442\u001B[0m         message\u001B[38;5;241m=\u001B[39mstatus\u001B[38;5;241m.\u001B[39mmessage,\n\u001B[1;32m   2443\u001B[0m         sql_state\u001B[38;5;241m=\u001B[39mErrorCode\u001B[38;5;241m.\u001B[39mCLIENT_UNEXPECTED_MISSING_SQL_STATE,  \u001B[38;5;66;03m# EDGE\u001B[39;00m\n\u001B[1;32m   2444\u001B[0m         grpc_status_code\u001B[38;5;241m=\u001B[39mstatus_code,\n\u001B[1;32m   2445\u001B[0m     ) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2446\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
        "\u001B[0;31mAnalysisException\u001B[0m: [TABLE_OR_VIEW_NOT_FOUND] The table or view `workspace`.`gold`.`dim_passengers` cannot be found. Verify the spelling and correctness of the schema and catalog.\nIf you did not qualify the name with a schema, verify the current_schema() output, or qualify the name with the correct schema and catalog.\nTo tolerate the error on drop use DROP VIEW IF EXISTS or DROP TABLE IF EXISTS. SQLSTATE: 42P01;\n'Project ['b.booking_id, 'b.booking_date, 'b.amount, 'f.dim_flights_key, 'p.dim_passengers_key, 'a.dim_airports_key, 'b.modifiedDate AS modify_date#14162]\n+- 'Join UsingJoin(LeftOuter, [airport_id])\n   :- 'Join UsingJoin(LeftOuter, [passenger_id])\n   :  :- Project [flight_id#14172, booking_id#14170, passenger_id#14171, airport_id#14173, amount#14174, booking_date#14175, modifiedDate#14176, dim_flights_key#14192L]\n   :  :  +- Join LeftOuter, (flight_id#14172 = flight_id#14186)\n   :  :     :- SubqueryAlias b\n   :  :     :  +- SubqueryAlias workspace.default.silver_bookings\n   :  :     :     +- Project [booking_id#14197 AS booking_id#14170, passenger_id#14198 AS passenger_id#14171, flight_id#14199 AS flight_id#14172, airport_id#14200 AS airport_id#14173, amount#14201 AS amount#14174, booking_date#14202 AS booking_date#14175, modifiedDate#14203 AS modifiedDate#14176]\n   :  :     :        +- View (`workspace`.`default`.`silver_bookings`, [booking_id#14197, passenger_id#14198, flight_id#14199, airport_id#14200, amount#14201, booking_date#14202, modifiedDate#14203])\n   :  :     :           +- Project [cast(booking_id#14204 as string) AS booking_id#14197, cast(passenger_id#14205 as string) AS passenger_id#14198, cast(flight_id#14206 as string) AS flight_id#14199, cast(airport_id#14207 as string) AS airport_id#14200, cast(amount#14208 as double) AS amount#14201, cast(booking_date#14209 as date) AS booking_date#14202, cast(modifiedDate#14210 as timestamp) AS modifiedDate#14203]\n   :  :     :              +- Project [booking_id#14204, passenger_id#14205, flight_id#14206, airport_id#14207, amount#14208, booking_date#14209, modifiedDate#14210]\n   :  :     :                 +- Relation workspace.default.silver_bookings[booking_id#14204,passenger_id#14205,flight_id#14206,airport_id#14207,amount#14208,booking_date#14209,modifiedDate#14210] parquet\n   :  :     +- SubqueryAlias f\n   :  :        +- Project [flight_id#14186, dim_flights_key#14192L]\n   :  :           +- SubqueryAlias workspace.gold.dim_flights\n   :  :              +- Relation workspace.gold.dim_flights[flight_id#14186,airline#14187,origin#14188,destination#14189,flight_date#14190,modifiedDate#14191,dim_flights_key#14192L,create_date#14193,update_date#14194] parquet\n   :  +- 'SubqueryAlias p\n   :     +- 'Project ['passenger_id, 'dim_passengers_key]\n   :        +- 'UnresolvedRelation [workspace, gold, dim_passengers], [], false\n   +- 'SubqueryAlias a\n      +- 'Project ['airport_id, 'dim_airports_key]\n         +- 'UnresolvedRelation [workspace, gold, dim_airports], [], false\n\n\nJVM stacktrace:\norg.apache.spark.sql.catalyst.ExtendedAnalysisException\n\tat org.apache.spark.sql.catalyst.analysis.package$AnalysisErrorAt.tableNotFound(package.scala:94)\n\tat org.apache.spark.sql.catalyst.analysis.RelationResolution$.throwTableNotFound(RelationResolution.scala:1017)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2(CheckAnalysis.scala:355)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis0$2$adapted(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:325)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$foreachUp$1$adapted(TreeNode.scala:324)\n\tat scala.collection.immutable.Vector.foreach(Vector.scala:2125)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.foreachUp(TreeNode.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0(CheckAnalysis.scala:324)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis0$(CheckAnalysis.scala:295)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis0(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.$anonfun$checkAnalysis$1(CheckAnalysis.scala:280)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis(CheckAnalysis.scala:267)\n\tat org.apache.spark.sql.catalyst.analysis.CheckAnalysis.checkAnalysis$(CheckAnalysis.scala:263)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.checkAnalysis(Analyzer.scala:557)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$resolveInFixedPoint$1(HybridAnalyzer.scala:420)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker$.withTracker(QueryPlanningTracker.scala:265)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.resolveInFixedPoint(HybridAnalyzer.scala:420)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.$anonfun$apply$1(HybridAnalyzer.scala:99)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.withTrackedAnalyzerBridgeState(HybridAnalyzer.scala:136)\n\tat org.apache.spark.sql.catalyst.analysis.resolver.HybridAnalyzer.apply(HybridAnalyzer.scala:92)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$2(Analyzer.scala:617)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:425)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.$anonfun$executeAndCheck$1(Analyzer.scala:617)\n\tat com.databricks.sql.unity.SAMSnapshotHelper$.visitPlansDuringAnalysis(SAMSnapshotHelper.scala:41)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:606)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$3(QueryExecution.scala:439)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.catalyst.QueryPlanningTracker.measurePhase(QueryPlanningTracker.scala:721)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$8(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withExecutionPhase$1(SQLExecution.scala:161)\n\tat com.databricks.logging.AttributionContext$.$anonfun$withValue$1(AttributionContext.scala:348)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat com.databricks.logging.AttributionContext$.withValue(AttributionContext.scala:344)\n\tat com.databricks.util.TracingSpanUtils$.$anonfun$withTracing$4(TracingSpanUtils.scala:235)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:129)\n\tat com.databricks.util.TracingSpanUtils$.withTracing(TracingSpanUtils.scala:233)\n\tat com.databricks.tracing.TracingUtils$.withTracing(TracingUtils.scala:296)\n\tat com.databricks.spark.util.DatabricksTracingHelper.withSpan(DatabricksSparkTracingHelper.scala:112)\n\tat com.databricks.spark.util.DBRTracing$.withSpan(DBRTracing.scala:47)\n\tat org.apache.spark.sql.execution.SQLExecution$.withExecutionPhase(SQLExecution.scala:142)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$7(QueryExecution.scala:961)\n\tat org.apache.spark.sql.execution.QueryExecution$.withInternalError(QueryExecution.scala:1620)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$5(QueryExecution.scala:954)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$4(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$3(QueryExecution.scala:951)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$2(QueryExecution.scala:950)\n\tat com.databricks.util.LexicalThreadLocal$Handle.runWith(LexicalThreadLocal.scala:63)\n\tat org.apache.spark.sql.execution.QueryExecution.withQueryExecutionId(QueryExecution.scala:938)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$executePhase$1(QueryExecution.scala:949)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.execution.QueryExecution.executePhase(QueryExecution.scala:948)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$2(QueryExecution.scala:421)\n\tat com.databricks.sql.util.MemoryTrackerHelper.withMemoryTracking(MemoryTrackerHelper.scala:111)\n\tat org.apache.spark.sql.execution.QueryExecution.$anonfun$lazyAnalyzed$1(QueryExecution.scala:420)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat org.apache.spark.util.Utils$.doTryWithCallerStacktrace(Utils.scala:1684)\n\tat org.apache.spark.util.LazyTry.tryT$lzycompute(LazyTry.scala:60)\n\tat org.apache.spark.util.LazyTry.tryT(LazyTry.scala:59)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:481)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:394)\n\tat org.apache.spark.sql.classic.Dataset$.$anonfun$ofRows$13(Dataset.scala:257)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.sql.classic.SparkSession.$anonfun$withActiveAndFrameProfiler$1(SparkSession.scala:1127)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat org.apache.spark.sql.classic.SparkSession.withActiveAndFrameProfiler(SparkSession.scala:1127)\n\tat org.apache.spark.sql.classic.Dataset$.ofRows(Dataset.scala:253)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.handleWriteOperation(SparkConnectPlanner.scala:4130)\n\tat org.apache.spark.sql.connect.planner.SparkConnectPlanner.process(SparkConnectPlanner.scala:3515)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.handleCommand(ExecuteThreadRunner.scala:394)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1(ExecuteThreadRunner.scala:290)\n\tat org.apache.spark.sql.connect.execution.ExecuteThreadRunner.$anonfun$executeInternal$1$adapted(ExecuteThreadRunner.scala:247)\n\tat org.apache.spark.sql.connect.service.SessionHolder.$anonfun$withSession$2(SessionHolder.scala:536)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:860)\n\tat org.apache.spark.util.Utils$.getTryWithCallerStacktrace(Utils.scala:1745)\n\tat org.apache.spark.util.LazyTry.get(LazyTry.scala:75)\n\tat org.apache.spark.sql.execution.QueryExecution.executedPlan(QueryExecution.scala:873)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$2(SqlExecutionMetrics.scala:199)\n\tat scala.Option.map(Option.scala:242)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.$anonfun$setQueryExecution$1(SqlExecutionMetrics.scala:199)\n\tat scala.util.Try$.apply(Try.scala:217)\n\tat com.databricks.spark.sqlgateway.history.SqlExecutionMetrics.setQueryExecution(SqlExecutionMetrics.scala:199)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onSqlStart$1(SqlGatewayHistorySparkListener.scala:870)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.com$databricks$spark$sqlgateway$history$SqlGatewayHistorySparkListener$$onSqlStart(SqlGatewayHistorySparkListener.scala:814)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:234)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener$$anonfun$onOtherEventDefault$1.applyOrElse(SqlGatewayHistorySparkListener.scala:222)\n\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat com.databricks.spark.sqlgateway.history.utils.ScriptStatementHelper$$anonfun$onOtherEvent$1.applyOrElse(ScriptStatementHelper.scala:28)\n\tat scala.PartialFunction$OrElse.applyOrElse(PartialFunction.scala:270)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.$anonfun$onOtherEvent$1(SqlGatewayHistorySparkListener.scala:200)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n\tat com.databricks.spark.util.FrameProfiler$.$anonfun$record$1(FrameProfiler.scala:114)\n\tat com.databricks.spark.util.FrameProfilerExporter$.maybeExportFrameProfiler(FrameProfilerExporter.scala:200)\n\tat com.databricks.spark.util.FrameProfiler$.record(FrameProfiler.scala:105)\n\tat com.databricks.spark.sqlgateway.history.SqlGatewayHistorySparkListener.onOtherEvent(SqlGatewayHistorySparkListener.scala:200)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent(SparkListenerBus.scala:108)\n\tat org.apache.spark.scheduler.SparkListenerBus.doPostEvent$(SparkListenerBus.scala:28)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.scheduler.AsyncEventQueue.doPostEvent(AsyncEventQueue.scala:46)\n\tat org.apache.spark.util.ListenerBus.postToAll(ListenerBus.scala:208)\n\tat org.apache.spark.util.ListenerBus.postToAll$(ListenerBus.scala:172)\n\tat org.apache.spark.scheduler.AsyncEventQueue.super$postToAll(AsyncEventQueue.scala:150)\n\tat org.apache.spark.scheduler.AsyncEventQueue.$anonfun$dispatch$1(AsyncEventQueue.scala:150)\n\tat scala.runtime.java8.JFunction0$mcJ$sp.apply(JFunction0$mcJ$sp.scala:17)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:59)\n\tat org.apache.spark.scheduler.AsyncEventQueue.org$apache$spark$scheduler$AsyncEventQueue$$dispatch(AsyncEventQueue.scala:119)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.$anonfun$run$1(AsyncEventQueue.scala:115)\n\tat org.apache.spark.util.Utils$.tryOrStopSparkContext(Utils.scala:1572)\n\tat org.apache.spark.scheduler.AsyncEventQueue$$anon$2.run(AsyncEventQueue.scala:115)"
       ],
       "type": "baseError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "catalog = \"workspace\"\n",
    "silver_schema = \"default\"  # Where your DLT created the tables\n",
    "gold_schema = \"gold\"       # Where we are writing\n",
    "\n",
    "# Table Names\n",
    "source_table = f\"{catalog}.{silver_schema}.silver_bookings\"\n",
    "fact_table = f\"{catalog}.{gold_schema}.fact_bookings\"\n",
    "\n",
    "# Dimension Tables to Join\n",
    "dim_flights = f\"{catalog}.{gold_schema}.dim_flights\"\n",
    "dim_passengers = f\"{catalog}.{gold_schema}.dim_passengers\"\n",
    "dim_airports = f\"{catalog}.{gold_schema}.dim_airports\"\n",
    "\n",
    "print(f\"Building Fact Table: {fact_table}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. READ DATA\n",
    "# ==========================================\n",
    "# Read the transactional data (Bookings)\n",
    "df_bookings = spark.table(source_table)\n",
    "\n",
    "# Read the Dimensions (We only need the Keys and the Natural IDs for joining)\n",
    "df_dim_flights = spark.table(dim_flights).select(\"flight_id\", \"dim_flights_key\")\n",
    "df_dim_passengers = spark.table(dim_passengers).select(\"passenger_id\", \"dim_passengers_key\")\n",
    "df_dim_airports = spark.table(dim_airports).select(\"airport_id\", \"dim_airports_key\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. JOINING (Replacing IDs with Surrogate Keys)\n",
    "# ==========================================\n",
    "# This creates the \"Star Schema\" by linking facts to dimensions\n",
    "df_fact = df_bookings.alias(\"b\") \\\n",
    "    .join(df_dim_flights.alias(\"f\"), \"flight_id\", \"left\") \\\n",
    "    .join(df_dim_passengers.alias(\"p\"), \"passenger_id\", \"left\") \\\n",
    "    .join(df_dim_airports.alias(\"a\"), \"airport_id\", \"left\") \\\n",
    "    .select(\n",
    "        col(\"b.booking_id\"),\n",
    "        col(\"b.booking_date\"),\n",
    "        col(\"b.amount\"),\n",
    "        col(\"f.dim_flights_key\"),\n",
    "        col(\"p.dim_passengers_key\"),\n",
    "        col(\"a.dim_airports_key\"),\n",
    "        col(\"b.modifiedDate\").alias(\"modify_date\") # Keeping lineage\n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 4. WRITE TO GOLD (Fact Table)\n",
    "# ==========================================\n",
    "# Fact tables are usually large, so we append or overwrite. \n",
    "# For this project, we will overwrite to ensure a clean state.\n",
    "\n",
    "print(\"Writing data to Gold Layer...\")\n",
    "\n",
    "df_fact.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table)\n",
    "\n",
    "print(f\"Success! Fact Table created at {fact_table}\")\n",
    "display(spark.sql(f\"SELECT * FROM {fact_table} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2eb6043f-2cbf-4410-af11-454e8cb4f70c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING BUILD: workspace.gold.dim_passengers...\nTable Created.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# --- CONFIGURATION FOR PASSENGERS ---\n",
    "key_columns_list = [\"passenger_id\"] \n",
    "cdc_column = \"modifiedDate\"\n",
    "source_table_path = \"workspace.default.silver_passengers\"\n",
    "target_table_path = \"workspace.gold.dim_passengers\"\n",
    "surrogate_key = \"dim_passengers_key\"\n",
    "\n",
    "print(f\"FORCING BUILD: {target_table_path}...\")\n",
    "\n",
    "# --- BUILDER LOGIC (Compact Version) ---\n",
    "# 1. Check Source\n",
    "try:\n",
    "    df_source = spark.table(source_table_path)\n",
    "except:\n",
    "    print(f\"Error: {source_table_path} missing. Run Silver pipeline.\")\n",
    "    dbutils.notebook.exit(\"Source missing\")\n",
    "\n",
    "# 2. Last Load Date\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    try:\n",
    "        mx = spark.sql(f\"SELECT MAX({cdc_column}) FROM {target_table_path}\").collect()[0][0]\n",
    "        last_load = mx if mx else \"1900-01-01\"\n",
    "    except: last_load = \"1900-01-01\"\n",
    "else:\n",
    "    last_load = \"1900-01-01\"\n",
    "\n",
    "# 3. Read & Join\n",
    "df_src = df_source.filter(col(cdc_column) > lit(last_load))\n",
    "df_src.createOrReplaceTempView(\"src_view\")\n",
    "\n",
    "if not spark.catalog.tableExists(target_table_path):\n",
    "    # Init Table Logic\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT CAST(NULL AS STRING) AS passenger_id, CAST(0 AS LONG) AS {surrogate_key}, \n",
    "        CAST(NULL AS TIMESTAMP) AS create_date, CAST(NULL AS TIMESTAMP) AS update_date WHERE 1=0\n",
    "    \"\"\").createOrReplaceTempView(\"tgt_view\")\n",
    "else:\n",
    "    spark.table(target_table_path).createOrReplaceTempView(\"tgt_view\")\n",
    "\n",
    "# 4. Filter New vs Old\n",
    "df_joined = spark.sql(f\"\"\"\n",
    "    SELECT s.*, t.{surrogate_key} as k, t.create_date as cd FROM src_view s \n",
    "    LEFT JOIN tgt_view t ON s.passenger_id = t.passenger_id\n",
    "\"\"\")\n",
    "df_new = df_joined.filter(\"k IS NULL\")\n",
    "df_old = df_joined.filter(\"k IS NOT NULL\")\n",
    "\n",
    "# 5. Generate Keys\n",
    "max_k = 0\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    r = spark.sql(f\"SELECT MAX({surrogate_key}) FROM {target_table_path}\").collect()[0][0]\n",
    "    if r: max_k = r\n",
    "\n",
    "df_new_final = df_new.withColumn(surrogate_key, (monotonically_increasing_id() + 1 + max_k).cast(\"long\")) \\\n",
    "    .withColumn(\"create_date\", current_timestamp()).withColumn(\"update_date\", current_timestamp()).drop(\"k\", \"cd\")\n",
    "df_old_final = df_old.withColumn(surrogate_key, col(\"k\").cast(\"long\")) \\\n",
    "    .withColumn(\"create_date\", col(\"cd\")).withColumn(\"update_date\", current_timestamp()).drop(\"k\", \"cd\")\n",
    "\n",
    "df_final = df_new_final.unionByName(df_old_final)\n",
    "\n",
    "# 6. Save\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    DeltaTable.forName(spark, target_table_path).alias(\"t\").merge(df_final.alias(\"s\"), f\"t.{surrogate_key}=s.{surrogate_key}\") \\\n",
    "        .whenMatchedUpdateAll(condition=f\"s.{cdc_column} >= t.{cdc_column}\").whenNotMatchedInsertAll().execute()\n",
    "    print(\"Upsert Done.\")\n",
    "else:\n",
    "    df_final.write.format(\"delta\").saveAsTable(target_table_path)\n",
    "    print(\"Table Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5bb234d4-9ea6-470f-8b58-e1785d998f3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FORCING BUILD: workspace.gold.dim_airports...\nTable Created.\n"
     ]
    }
   ],
   "source": [
    "# --- CONFIGURATION FOR AIRPORTS ---\n",
    "key_columns_list = [\"airport_id\"] \n",
    "cdc_column = \"modifiedDate\"\n",
    "source_table_path = \"workspace.default.silver_airports\"\n",
    "target_table_path = \"workspace.gold.dim_airports\"\n",
    "surrogate_key = \"dim_airports_key\"\n",
    "\n",
    "print(f\"FORCING BUILD: {target_table_path}...\")\n",
    "\n",
    "# (Re-running the exact same logic as above with these new variables)\n",
    "# ... [Logic is identical to Step 1, this block ensures variables update] ...\n",
    "\n",
    "# 1. Check Source\n",
    "try:\n",
    "    df_source = spark.table(source_table_path)\n",
    "except:\n",
    "    dbutils.notebook.exit(\"Source missing\")\n",
    "\n",
    "# 2. Last Load\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    try:\n",
    "        mx = spark.sql(f\"SELECT MAX({cdc_column}) FROM {target_table_path}\").collect()[0][0]\n",
    "        last_load = mx if mx else \"1900-01-01\"\n",
    "    except: last_load = \"1900-01-01\"\n",
    "else:\n",
    "    last_load = \"1900-01-01\"\n",
    "\n",
    "# 3. Read & Join\n",
    "df_src = df_source.filter(col(cdc_column) > lit(last_load))\n",
    "df_src.createOrReplaceTempView(\"src_view\")\n",
    "\n",
    "if not spark.catalog.tableExists(target_table_path):\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT CAST(NULL AS STRING) AS airport_id, CAST(0 AS LONG) AS {surrogate_key}, \n",
    "        CAST(NULL AS TIMESTAMP) AS create_date, CAST(NULL AS TIMESTAMP) AS update_date WHERE 1=0\n",
    "    \"\"\").createOrReplaceTempView(\"tgt_view\")\n",
    "else:\n",
    "    spark.table(target_table_path).createOrReplaceTempView(\"tgt_view\")\n",
    "\n",
    "# 4. Filter\n",
    "df_joined = spark.sql(f\"\"\"\n",
    "    SELECT s.*, t.{surrogate_key} as k, t.create_date as cd FROM src_view s \n",
    "    LEFT JOIN tgt_view t ON s.airport_id = t.airport_id\n",
    "\"\"\")\n",
    "df_new = df_joined.filter(\"k IS NULL\")\n",
    "df_old = df_joined.filter(\"k IS NOT NULL\")\n",
    "\n",
    "# 5. Generate Keys\n",
    "max_k = 0\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    r = spark.sql(f\"SELECT MAX({surrogate_key}) FROM {target_table_path}\").collect()[0][0]\n",
    "    if r: max_k = r\n",
    "\n",
    "df_new_final = df_new.withColumn(surrogate_key, (monotonically_increasing_id() + 1 + max_k).cast(\"long\")) \\\n",
    "    .withColumn(\"create_date\", current_timestamp()).withColumn(\"update_date\", current_timestamp()).drop(\"k\", \"cd\")\n",
    "df_old_final = df_old.withColumn(surrogate_key, col(\"k\").cast(\"long\")) \\\n",
    "    .withColumn(\"create_date\", col(\"cd\")).withColumn(\"update_date\", current_timestamp()).drop(\"k\", \"cd\")\n",
    "\n",
    "df_final = df_new_final.unionByName(df_old_final)\n",
    "\n",
    "# 6. Save\n",
    "if spark.catalog.tableExists(target_table_path):\n",
    "    DeltaTable.forName(spark, target_table_path).alias(\"t\").merge(df_final.alias(\"s\"), f\"t.{surrogate_key}=s.{surrogate_key}\") \\\n",
    "        .whenMatchedUpdateAll(condition=f\"s.{cdc_column} >= t.{cdc_column}\").whenNotMatchedInsertAll().execute()\n",
    "    print(\"Upsert Done.\")\n",
    "else:\n",
    "    df_final.write.format(\"delta\").saveAsTable(target_table_path)\n",
    "    print(\"Table Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "614ba6a9-e660-48aa-b122-ec05282c0816",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building Fact Table: workspace.gold.fact_bookings\nRead Bookings Table.\nWriting to Gold Layer...\nSuccess! Fact Table created at workspace.gold.fact_bookings\n"
     ]
    },
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>booking_id</th><th>booking_date</th><th>amount</th><th>dim_flights_key</th><th>dim_passengers_key</th><th>dim_airports_key</th><th>modify_date</th></tr></thead><tbody><tr><td>B00001</td><td>2025-05-29</td><td>850.72</td><td>14</td><td>48</td><td>48</td><td>2026-02-09T06:19:14.396Z</td></tr><tr><td>B00002</td><td>2025-06-09</td><td>376.63</td><td>52</td><td>11</td><td>3</td><td>2026-02-09T06:19:14.396Z</td></tr><tr><td>B00003</td><td>2025-06-03</td><td>534.02</td><td>23</td><td>79</td><td>12</td><td>2026-02-09T06:19:14.396Z</td></tr><tr><td>B00004</td><td>2025-06-16</td><td>1333.7</td><td>1</td><td>68</td><td>39</td><td>2026-02-09T06:19:14.396Z</td></tr><tr><td>B00005</td><td>2025-06-17</td><td>1334.96</td><td>19</td><td>189</td><td>8</td><td>2026-02-09T06:19:14.396Z</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "B00001",
         "2025-05-29",
         850.72,
         14,
         48,
         48,
         "2026-02-09T06:19:14.396Z"
        ],
        [
         "B00002",
         "2025-06-09",
         376.63,
         52,
         11,
         3,
         "2026-02-09T06:19:14.396Z"
        ],
        [
         "B00003",
         "2025-06-03",
         534.02,
         23,
         79,
         12,
         "2026-02-09T06:19:14.396Z"
        ],
        [
         "B00004",
         "2025-06-16",
         1333.7,
         1,
         68,
         39,
         "2026-02-09T06:19:14.396Z"
        ],
        [
         "B00005",
         "2025-06-17",
         1334.96,
         19,
         189,
         8,
         "2026-02-09T06:19:14.396Z"
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "booking_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "booking_date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "amount",
         "type": "\"double\""
        },
        {
         "metadata": "{}",
         "name": "dim_flights_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dim_passengers_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "dim_airports_key",
         "type": "\"long\""
        },
        {
         "metadata": "{}",
         "name": "modify_date",
         "type": "\"timestamp\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from delta.tables import *\n",
    "\n",
    "# ==========================================\n",
    "# 1. CONFIGURATION\n",
    "# ==========================================\n",
    "catalog = \"workspace\"\n",
    "silver_schema = \"default\"  # Where your DLT pipeline put the tables\n",
    "gold_schema = \"gold\"       # Where we are writing\n",
    "\n",
    "# Source Table (Bookings)\n",
    "source_table = f\"{catalog}.{silver_schema}.silver_bookings\"\n",
    "\n",
    "# Target Table (Fact)\n",
    "fact_table = f\"{catalog}.{gold_schema}.fact_bookings\"\n",
    "\n",
    "# Dimension Tables to Join (These must exist now!)\n",
    "dim_flights = f\"{catalog}.{gold_schema}.dim_flights\"\n",
    "dim_passengers = f\"{catalog}.{gold_schema}.dim_passengers\"\n",
    "dim_airports = f\"{catalog}.{gold_schema}.dim_airports\"\n",
    "\n",
    "print(f\"Building Fact Table: {fact_table}\")\n",
    "\n",
    "# ==========================================\n",
    "# 2. READ DATA\n",
    "# ==========================================\n",
    "# Read the transactional data\n",
    "try:\n",
    "    df_bookings = spark.table(source_table)\n",
    "    print(\"Read Bookings Table.\")\n",
    "except:\n",
    "    print(f\"Error: Could not find {source_table}\")\n",
    "    dbutils.notebook.exit(\"Source missing\")\n",
    "\n",
    "# Read the Dimensions\n",
    "# We only need the Natural Keys (ID) and Surrogate Keys (Key)\n",
    "df_dim_flights = spark.table(dim_flights).select(\"flight_id\", \"dim_flights_key\")\n",
    "df_dim_passengers = spark.table(dim_passengers).select(\"passenger_id\", \"dim_passengers_key\")\n",
    "df_dim_airports = spark.table(dim_airports).select(\"airport_id\", \"dim_airports_key\")\n",
    "\n",
    "# ==========================================\n",
    "# 3. JOINING (The \"Star Schema\" Logic)\n",
    "# ==========================================\n",
    "# This replaces the text IDs with your new Gold Surrogate Keys\n",
    "df_fact = df_bookings.alias(\"b\") \\\n",
    "    .join(df_dim_flights.alias(\"f\"), \"flight_id\", \"left\") \\\n",
    "    .join(df_dim_passengers.alias(\"p\"), \"passenger_id\", \"left\") \\\n",
    "    .join(df_dim_airports.alias(\"a\"), \"airport_id\", \"left\") \\\n",
    "    .select(\n",
    "        col(\"b.booking_id\"),\n",
    "        col(\"b.booking_date\"),\n",
    "        col(\"b.amount\"),\n",
    "        col(\"f.dim_flights_key\"),\n",
    "        col(\"p.dim_passengers_key\"),\n",
    "        col(\"a.dim_airports_key\"),\n",
    "        col(\"b.modifiedDate\").alias(\"modify_date\") \n",
    "    )\n",
    "\n",
    "# ==========================================\n",
    "# 4. WRITE TO GOLD\n",
    "# ==========================================\n",
    "print(\"Writing to Gold Layer...\")\n",
    "\n",
    "df_fact.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .saveAsTable(fact_table)\n",
    "\n",
    "print(f\"Success! Fact Table created at {fact_table}\")\n",
    "display(spark.sql(f\"SELECT * FROM {fact_table} LIMIT 5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d522bfd2-35dc-4d5d-aa37-df038ce6a0d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8060742587403117,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "GOLD_DIMS",
   "widgets": {
    "backdated_refresh": {
     "currentValue": "",
     "nuid": "6b00ada2-1265-4d22-9da9-ea98493ad124",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": null,
      "name": "backdated_refresh",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": null,
      "name": "backdated_refresh",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "catalog": {
     "currentValue": "workspace",
     "nuid": "811279d9-1c49-4ad6-9379-15243a29fdac",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "workspace",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "workspace",
      "label": null,
      "name": "catalog",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "cdc_column": {
     "currentValue": "modify_date",
     "nuid": "964daaec-08de-4cb6-8767-f18cf32484b4",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "modify_date",
      "label": null,
      "name": "cdc_column",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "modify_date",
      "label": null,
      "name": "cdc_column",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "key_columns": {
     "currentValue": "['flight_id']",
     "nuid": "a1ea4d88-7373-45a7-ba31-be1d844ac690",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "['flight_id']",
      "label": null,
      "name": "key_columns",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "['flight_id']",
      "label": null,
      "name": "key_columns",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_object": {
     "currentValue": "silver_flights",
     "nuid": "78f24d23-da4f-4b38-bf06-9ec7e90013b0",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "silver_flights",
      "label": null,
      "name": "source_object",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "silver_flights",
      "label": null,
      "name": "source_object",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "source_schema": {
     "currentValue": "default",
     "nuid": "2f139de1-3bd5-4751-a54d-af0e8b7a8931",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": null,
      "name": "source_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "default",
      "label": null,
      "name": "source_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "surrogate_key": {
     "currentValue": "dim_flights_key",
     "nuid": "416db0b1-3e0c-4c2f-bc6a-9b7e3ffac987",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dim_flights_key",
      "label": null,
      "name": "surrogate_key",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dim_flights_key",
      "label": null,
      "name": "surrogate_key",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_object": {
     "currentValue": "dim_flights",
     "nuid": "7e5cc3d7-ae5a-4941-99a1-aaf1863f4474",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "dim_flights",
      "label": null,
      "name": "target_object",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "dim_flights",
      "label": null,
      "name": "target_object",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    },
    "target_schema": {
     "currentValue": "gold",
     "nuid": "a2d9b525-b078-4704-908c-2aa527b45ea1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "gold",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "gold",
      "label": null,
      "name": "target_schema",
      "options": {
       "widgetType": "text",
       "autoCreated": null,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}